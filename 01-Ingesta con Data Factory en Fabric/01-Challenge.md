# ğŸ† Reto 1: Ingesta de Datos desde Cosmos DB a Microsoft Fabric (Capa Bronze) + Limpieza BÃ¡sica  

ğŸ“– Escenario  
Contoso necesita consolidar sus **datos operativos y financieros** en **Microsoft Fabric**.  
El equipo de datos debe realizar la **ingesta desde Azure Cosmos DB** hacia la capa **Bronze** y aplicar una **limpieza minima inicial** para preparar los datos antes de avanzar a las siguientes fases de transformaciÃ³n.  

---

### ğŸ¯ Tu MisiÃ³n  
Al completar este reto podrÃ¡s:  

âœ… Ingerir los datos desde **Azure Cosmos DB** hacia **Microsoft Fabric** utilizando **Dataflows Gen2**.  
âœ… Aplicar **limpieza bÃ¡sica** que incluya por ejemplo:  
- Manejo de valores nulos o vacÃ­os.  
- EliminaciÃ³n de columnas innecesarias.  
- NormalizaciÃ³n de formatos bÃ¡sicos (fechas, texto, etc.)
  
âœ… Generar una capa semi/cruda dentro de **Bronze** del Lakehouse.  

---

## ğŸš€ Paso 1: Crear un Dataflow Gen2 para la Ingesta desde Cosmos DB (Pueden utilizar otros metodos de ingesta)
ğŸ’¡ *Â¿Por quÃ©?* Los **Dataflows Gen2** permiten realizar la ingesta y transformaciÃ³n inicial de datos sin necesidad de cÃ³digo, conectando fÃ¡cilmente fuentes externas como Cosmos DB con tu Lakehouse. Opcional: Esta ingesta es posible realizarla tambien desde un **Pipeline** con actividades de copia, **Notebooks**, o incluso con la opcion de **Mirroring de Cosmos DB** que permite exponer los contenedores Cosmos en Fabric

1ï¸âƒ£ En **Microsoft Fabric**, crea un nuevo **Dataflow Gen2** dentro de tu workspace.  
2ï¸âƒ£ Selecciona **Azure Cosmos DB** como fuente de datos.  
3ï¸âƒ£ Ingresa las credenciales de conexiÃ³n (endpoint y clave de acceso).  
4ï¸âƒ£ Conecta con el contenedor que contiene los datos de **productos** , **credit score** y **transacciones**.  
5ï¸âƒ£ Define como destino tu **Lakehouse** schema [bronze] para almacenar los datos ingestados (asegurate de activar el flag TRUE de *`navigate full hierarchy`* desde opciones avanzadas de la conexion al Lakehouse, esto permite navegar sobre schemas).  

âœ… **Resultado esperado:** Los datos JSON de Cosmos DB se encuentran disponibles en los query de Dataflow Gen2

## ğŸš€ Paso 2: Aplicar Limpiezas BÃ¡sicas en el Dataflow Gen2  
ğŸ’¡ *Â¿Por quÃ©?* Este paso mejora la calidad de los datos, asegurando consistencia y usabilidad para anÃ¡lisis posteriores. Es normal que algunas organizaciones realicen limpiezas en Bronze, otras ingestan totalmente los datos en crudo para posteriormente prepararlo.  
1ï¸âƒ£ Edita tu **Dataflow Gen2** para agregar pasos de transformaciÃ³n:  
   - ğŸ§¹ **Eliminar columnas innecesarias** que no aporten valor analÃ­tico.  
   - ğŸ©¹ **Reemplazar o eliminar valores nulos o vacÃ­os.**  
   - ğŸ•’ **Normalizar formatos bÃ¡sicos** (por ejemplo, campos de fecha o texto en minÃºsculas).
      
2ï¸âƒ£ Guarda y ejecuta el Dataflow para aplicar las transformaciones.  
3ï¸âƒ£ Publica los resultados en la **capa Bronze** de tu Lakehouse.  

âœ… **Resultado esperado:** Las tablas â€œBronzeâ€ contiene datos limpios, y listos para su transformaciÃ³n en la capa Silver.  

---

## ğŸš€ Paso 3: Validar la Carga y Estructura de los Datos  
ğŸ’¡ *Â¿Por quÃ©?* Validar la ingesta garantiza que los datos sean completos y coherentes antes de iniciar la limpieza.  

1ï¸âƒ£ Accede a tu **Lakehouse** desde el panel de Fabric.  
ğŸ”¹ Revisa que las tablas delta creadas contengan los campos esperados.  
ğŸ”¹ Comprueba que no existan errores de formato o registros incompletos.  

âœ… **Resultado esperado:** La estructura base de los datos ha sido validada correctamente.  

---


## ğŸ Puntos de Control Finales  

âœ… Â¿Se completÃ³ la ingesta desde Cosmos DB mediante Dataflows Gen2?  
âœ… Â¿Se aplicaron correctamente las limpiezas bÃ¡sicas?  
âœ… Â¿Los datos resultantes estÃ¡n almacenados y accesibles en la capa Bronze?  
âœ… Â¿Se documentaron los pasos realizados y las evidencias visuales?  

---

## ğŸ“ DocumentaciÃ³n  


- [Creacion Dataflow Gen2](https://learn.microsoft.com/es-mx/fabric/data-factory/create-first-dataflow-gen2)




